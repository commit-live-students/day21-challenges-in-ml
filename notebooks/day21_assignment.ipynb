{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation Note:** For each task, the sections **Solution** and **Validating student's solution** are not meant to be seen by the student before attempting the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**: This assignment contains exercises on various methods to handle imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Imbalanced pima-indians dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting\n",
    "\n",
    "We have gone through `pima-indians` dataset before. Here is an artifically imbanced pima-indian dataset.\n",
    "\n",
    "* We have discussed that using accuracy is not the best way to measure the performance of an algorithm in case of imbalnced datasets. \n",
    "* Hence, we will be using cohen's kappa along with accuracy to get a more all-round picture of the performance.\n",
    "* With Base XGBoost model, we are getting \n",
    "    - Accuracy: 84.34%\n",
    "    - Kappa: 35.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jay/miniconda3/envs/py27/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.datasets import make_imbalance\n",
    "from numpy import loadtxt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = loadtxt('./data/pima-indians-diabetes.csv', delimiter=\",\")\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = make_imbalance(X, Y, ratio=0.20, min_c_=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgbmodel = XGBClassifier(seed=42)\n",
    "xgbmodel.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.34%\n",
      "Kappa: 35.32\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgbmodel.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"Kappa: %.2f\" % (kappa * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Handling Imbalanced Data with Algorithmic Appraoch\n",
    "\n",
    "* Your task is to achieve at least 45% kappa with 84%+ accurcy on the given X_train, X_test, y_train, y_test. However, \n",
    "    * You are only allowed to use the `XGBoost` model. \n",
    "    * You are only allowed to change at most one hyperparameter.\n",
    "\n",
    "You need to write a function called `myImbalanced()` that\n",
    "\n",
    "* Accepts the following parameters:\n",
    "    - X_train, X_test,  y_train, y_test (Numpy arrays for training, testing; any format acceptable by sklearn will work)\n",
    "    - **kwargs (Accepts the model parameter that you wish to change)\n",
    "  \n",
    "\n",
    "* Should return\n",
    "    - Accuracy on test data\n",
    "    - Cohen's kappa on test data\n",
    "    - Trained model\n",
    "    \n",
    "    \n",
    "**Note**: \n",
    "\n",
    "1. Keep seed=42, random_state=42 wherever applicable.\n",
    "1. [Imp] Since, the function only accepts final parameter, you might have run a few experiments before submitting the final answer.\n",
    "\n",
    "**Hint**:\n",
    "1. If I were you, I would probably try to recall what we discussed in 'algorithmic approach'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myImbalanced(X_train, X_test, y_train, y_test, **kwargs):\n",
    "    \n",
    "    model = XGBClassifier(seed=42)\n",
    "    model.set_params(**kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    return accuracy, kappa, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myImbalanced_acc, myImbalanced_kappa, myImbalanced_model =\\\n",
    "    myImbalanced(X_train, X_test, y_train, y_test, scale_pos_weight=3.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.843434343434\n",
      "kappa: 0.455947527034\n"
     ]
    }
   ],
   "source": [
    "print \"accuracy\", myImbalanced_acc\n",
    "print \"kappa:\", myImbalanced_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=3.6, seed=42, silent=True, subsample=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myImbalanced_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Note**: For the remaining exercises, do not tune hyperparameters of the XGBoost model, unless advised otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Implement Oversampling with SMOTE as a part of ML pipeline\n",
    "\n",
    "* Next, we will try to tackle the same dataset using oversamping technique SMOTE.\n",
    "* You need to write a function that applies smote to the dataset, and then trains it on XGBoost model.\n",
    "\n",
    "* Write a function `SMOTEpipeline` that\n",
    "    * Accepts no parameters:\n",
    "    * Should return\n",
    "        - Discussed pipleline object\n",
    "    \n",
    "**Note**: \n",
    "\n",
    "1. keep seed=42, random_state=42 wherever applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SMOTEpipeline():\n",
    "    smote = SMOTE(random_state=42)\n",
    "    model = XGBClassifier(seed=42)\n",
    "    pipeline = make_pipeline(smote, model)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SMOTE_pipeline = SMOTEpipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smote': SMOTE(k=None, k_neighbors=5, kind='regular', m=None, m_neighbors=10, n_jobs=1,\n",
       "    out_step=0.5, random_state=42, ratio='auto', svm_estimator=None),\n",
       " 'xgbclassifier': XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "        gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "        min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "        objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "        scale_pos_weight=1, seed=42, silent=True, subsample=1)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMOTE_pipeline.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Implement Undersampling with Edited Nearest Neighbours as a part of ML pipeline\n",
    "\n",
    "* Next, we will try to tackle the same dataset using oversamping technique SMOTE.\n",
    "* You need to write a function that applies smote to the dataset, and then trains it on XGBoost model.\n",
    "* Write a function `ENNpipeline` that\n",
    "    * Accepts no parameters:\n",
    "    * Should return\n",
    "        - Discussed pipleline object\n",
    "    \n",
    "**Note**: \n",
    "\n",
    "1. keep seed=42, random_state=42 wherever applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, RepeatedEditedNearestNeighbours\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ENNpipeline():\n",
    "    enn = EditedNearestNeighbours(random_state=42)\n",
    "    model = XGBClassifier(seed=42)\n",
    "    pipeline = make_pipeline(enn, model)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENN_pipeline = ENNpipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'editednearestneighbours': EditedNearestNeighbours(kind_sel='all', n_jobs=1, n_neighbors=3,\n",
       "             random_state=42, ratio='auto', return_indices=False,\n",
       "             size_ngh=None),\n",
       " 'xgbclassifier': XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "        gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "        min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "        objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "        scale_pos_weight=1, seed=42, silent=True, subsample=1)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENN_pipeline.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Implement Grid Search on the Pipeline Object\n",
    "\n",
    "* You need to run `GridSearchCV` on the `pipeline` object from the previous exercise in order to figure out the best parameters for SMOTE oversampling.\n",
    "\n",
    "* You need to write a function called `myGridSearch()` that\n",
    "    * Accepts the following parameters:\n",
    "        - X_train, X_test,  y_train, y_test (Numpy arrays for training, testing; any format acceptable by sklearn will work)\n",
    "        - pipeline object\n",
    "        - paramsgrid (for GridSearchCV)\n",
    "    * Should return\n",
    "        - Accuracy of test set\n",
    "        - Kappa of test set\n",
    "        - classification report of test set\n",
    "        - Trained GridSearchCV model\n",
    "    \n",
    "**Note**: \n",
    "\n",
    "1. keep seed=42, random_state=42 wherever applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "def myGridSearch(X_train, X_test, y_train, y_test, pipeline, paramgrid):\n",
    "    \n",
    "#     print pipeline\n",
    "#     print paramgrid\n",
    "\n",
    "    gridsearch = GridSearchCV(pipeline, param_grid=paramgrid, cv=3)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    y_pred = gridsearch.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    clf_report = classification_report_imbalanced(y_test, y_pred)\n",
    "    return accuracy, kappa, clf_report, gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myGridSearch_pipeline = ENNpipeline()\n",
    "myGridSearch_paramgrid = {\"editednearestneighbours__n_neighbors\": [2,3,4,5,6,7,8]}\n",
    "\n",
    "myGridSearch_acc, myGridSearch_kappa, myGridSearch_clf_report, myGridSearch_params = \\\n",
    "myGridSearch(X_train, X_test, y_train, y_test, myGridSearch_pipeline, myGridSearch_paramgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85353535353535348"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myGridSearch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5234854771784232"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myGridSearch_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "        0.0       0.92      0.90      0.66      0.91      0.73      0.55       163\n",
      "        1.0       0.57      0.66      0.90      0.61      0.73      0.51        35\n",
      "\n",
      "avg / total       0.86      0.85      0.70      0.86      0.73      0.54       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print myGridSearch_clf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EditedNearestNeighbours(kind_sel='all', n_jobs=1, n_neighbors=3,\n",
       "            random_state=42, ratio='auto', return_indices=False,\n",
       "            size_ngh=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myGridSearch_params.best_estimator_.steps[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=42, silent=True, subsample=1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myGridSearch_params.best_estimator_.steps[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Figure out optimum `k_neighbours` values for `SMOTEpipeline`:\n",
    "\n",
    "* You need to achieve similar results as the previous exercise (45%+ kappa with 84%+ accurcy).\n",
    "\n",
    "You need to write a function called `k_neighbours()` that\n",
    "\n",
    "* Accepts the following parameters:\n",
    "    - X_train, X_test,  y_train, y_test (Numpy arrays for training, testing; any format acceptable by sklearn will work)\n",
    "* Should return\n",
    "    - Accuracy of test set\n",
    "    - Kappa of test set\n",
    "    - classification report of test set\n",
    "    - Trained gridsearchCV model for `k_neighbours`\n",
    "    \n",
    "**Note**: \n",
    "\n",
    "1. keep seed=42, random_state=42 wherever applicable.\n",
    "2. You may need to hardcode the parameter grid in your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_neighbours(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    k_neighbours_paramsgrid = {\"smote__k_neighbors\": [2,3,4,5,6,7,8]}\n",
    "    return myGridSearch(X_train, X_test, y_train, y_test, SMOTEpipeline(), k_neighbours_paramsgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_neighbours_acc, k_neighbours_kappa, k_neighbours_clf_report, k_neighbours_grid = \\\n",
    "    k_neighbours(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787878787879\n",
      "0.317129249466\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "        0.0       0.89      0.85      0.49      0.87      0.61      0.38       163\n",
      "        1.0       0.41      0.49      0.85      0.45      0.61      0.35        35\n",
      "\n",
      "avg / total       0.80      0.79      0.55      0.79      0.61      0.38       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print k_neighbours_acc\n",
    "print k_neighbours_kappa\n",
    "print k_neighbours_clf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE(k=None, k_neighbors=2, kind='regular', m=None, m_neighbors=10, n_jobs=1,\n",
      "   out_step=0.5, random_state=42, ratio='auto', svm_estimator=None)\n"
     ]
    }
   ],
   "source": [
    "print k_neighbours_grid.best_estimator_.steps[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Figure out optimum `kind` for `SMOTEpipeline`:\n",
    "\n",
    "* You need to achieve similar results as the previous exercise (45%+ kappa with 84%+ accurcy ).\n",
    "\n",
    "You need to write a function called `kind()` that\n",
    "\n",
    "* Accepts the following parameters:\n",
    "    - X_train, X_test,  y_train, y_test (Numpy arrays for training, testing; any format acceptable by sklearn will work)\n",
    "* Should return\n",
    "    - Accuracy of test set\n",
    "    - Kappa of test set\n",
    "    - classification report of test set\n",
    "    - Trained gridsearchCV model for optimum `kind` of smote\n",
    "    \n",
    "**Note**: \n",
    "\n",
    "1. keep seed=42, random_state=42 wherever applicable.\n",
    "2. You may need to hardcode the parameter grid in your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kind(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    kind_paramsgrid = {\"smote__kind\": ['regular', 'borderline1', 'borderline2', 'svm']}\n",
    "    return myGridSearch(X_train, X_test, y_train, y_test, SMOTEpipeline(), kind_paramsgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kind_acc, kind_kappa, kind_clf_report, kind_grid = kind(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843434343434\n",
      "0.468018720749\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "        0.0       0.91      0.90      0.57      0.90      0.71      0.52       163\n",
      "        1.0       0.56      0.57      0.90      0.56      0.71      0.49        35\n",
      "\n",
      "avg / total       0.85      0.84      0.63      0.84      0.71      0.52       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print kind_acc\n",
    "print kind_kappa\n",
    "print kind_clf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE(k=None, k_neighbors=5, kind='svm', m=None, m_neighbors=10, n_jobs=1,\n",
      "   out_step=0.5, random_state=42, ratio='auto', svm_estimator=None)\n"
     ]
    }
   ],
   "source": [
    "print kind_grid.best_estimator_.steps[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Finetune both `kind` and `k_neighbour` for `SMOTEpipeline`:\n",
    "\n",
    "* You need to achieve similar results as the previous exercise (45%+ kappa with 84%+ accurcy ).\n",
    "* You need to write a function called `optimum()` that\n",
    "\n",
    "    * Accepts the following parameters:\n",
    "        - X_train, X_test,  y_train, y_test (Numpy arrays for training, testing; any format acceptable by sklearn will work)\n",
    "    * Should return\n",
    "        - Accuracy of test set\n",
    "        - Kappa of test set\n",
    "        - classification report of test set\n",
    "        - Trained gridsearchCV object for optimum `kind` and `k_neighbour` combination.\n",
    "    \n",
    "**Note**: \n",
    "\n",
    "1. keep seed=42, random_state=42 wherever applicable.\n",
    "2. You may need to hardcode the parameter grid in your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimum(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    optimum_paramsgrid = {\"smote__k_neighbors\": [2,3,4,5,6,7,8],\n",
    "                  \"smote__kind\": ['regular', 'borderline1', 'borderline2', 'svm']}\n",
    "    return myGridSearch(X_train, X_test, y_train, y_test, SMOTEpipeline(), optimum_paramsgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimum_acc, optimum_kappa, optimum_clf_report, optimum_grid = optimum(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843434343434\n",
      "0.468018720749\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "        0.0       0.91      0.90      0.57      0.90      0.71      0.52       163\n",
      "        1.0       0.56      0.57      0.90      0.56      0.71      0.49        35\n",
      "\n",
      "avg / total       0.85      0.84      0.63      0.84      0.71      0.52       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print optimum_acc\n",
    "print optimum_kappa\n",
    "print optimum_clf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE(k=None, k_neighbors=5, kind='svm', m=None, m_neighbors=10, n_jobs=1,\n",
      "   out_step=0.5, random_state=42, ratio='auto', svm_estimator=None)\n"
     ]
    }
   ],
   "source": [
    "print optimum_grid.best_estimator_.steps[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Figure out optimum `n_neighbour` for `ENNpipeline`:\n",
    "\n",
    "* You need to achieve similar results as the previous exercise (45%+ kappa with 84%+ accurcy ).\n",
    "* You need to write a function called `n_neighbour()` that\n",
    "\n",
    "    * Accepts the following parameters:\n",
    "        - X_train, X_test,  y_train, y_test (Numpy arrays for training, testing; any format acceptable by sklearn will work)\n",
    "    * Should return\n",
    "        - Accuracy of test set\n",
    "        - Kappa of test set\n",
    "        - classification report of test set\n",
    "        - Trained gridsearchCV object for optimum `kind` and `k_neighbour` combination.\n",
    "    \n",
    "**Note**: \n",
    "\n",
    "1. keep seed=42, random_state=42 wherever applicable.\n",
    "2. You may need to hardcode the parameter grid in your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_neighbour(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    n_neighbour_paramsgrid = {\"editednearestneighbours__n_neighbors\": [2,3,4,5,6,7,8]}\n",
    "    return myGridSearch(X_train, X_test, y_train, y_test, ENNpipeline(), n_neighbour_paramsgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_neighbour_acc, n_neighbour_kappa, n_neighbour_clf_report, n_neighbour_grid = n_neighbour(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.853535353535\n",
      "0.523485477178\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "        0.0       0.92      0.90      0.66      0.91      0.73      0.55       163\n",
      "        1.0       0.57      0.66      0.90      0.61      0.73      0.51        35\n",
      "\n",
      "avg / total       0.86      0.85      0.70      0.86      0.73      0.54       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print n_neighbour_acc\n",
    "print n_neighbour_kappa\n",
    "print n_neighbour_clf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EditedNearestNeighbours(kind_sel='all', n_jobs=1, n_neighbors=3,\n",
      "            random_state=42, ratio='auto', return_indices=False,\n",
      "            size_ngh=None)\n"
     ]
    }
   ],
   "source": [
    "print n_neighbour_grid.best_estimator_.steps[0][1]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:greyatom]",
   "language": "python",
   "name": "conda-env-greyatom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
